{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "如果你有超过BERT模型最大序列长度（512个tokens）的代码片段，你将需要采取一些策略来处理这个限制。以下是一些可行的方法：\n",
    "\n",
    "1. 分块（Chunking）\n",
    "将代码分成多个块，每个块都小于或等于最大序列长度。每个块可以单独处理，然后可以采取不同的策略来整合块的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippet = \"\"\"\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def read_file(filename):\n",
    "   \n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            return file.read()\n",
    "    except IOError:\n",
    "        print(f\"Error opening {filename}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def process_content(content):\n",
    "    \n",
    "    processed_content = content.upper()\n",
    "    return processed_content\n",
    "\n",
    "def write_file(filename, content):\n",
    "   \n",
    "    try:\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(content)\n",
    "    except IOError:\n",
    "        print(f\"Error writing to {filename}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: python script.py input_file output_file\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    input_file = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    \n",
    "    # Read the input file\n",
    "    content = read_file(input_file)\n",
    "    \n",
    "    # Process the content\n",
    "    processed_content = process_content(content)\n",
    "    \n",
    "    # Write the processed content to the output file\n",
    "    write_file(output_file, processed_content)\n",
    "    \n",
    "    print(f\"Processed content written to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "max_length = 512  # BERT's maximum sequence length\n",
    "\n",
    "# The actual code snippet is now set in the `code_snippet` variable and exceeds typical length limits, \n",
    "# requiring the sliding window approach for processing with models like CodeBERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "model.eval()  # 设置模型为评估模式\n",
    "chunk_size = 512  # 每个chunk的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这段代码首先将代码分割成多个chunk，每个chunk由500个token组成。\n",
    "# 然后，对每个chunk独立地获取embedding，并计算这些embeddings的平均值以获得代表整个代码段的单个embedding向量。\n",
    "# 这种方法允许您处理超过模型最大序列长度限制的长代码段，并且通过平均embeddings来捕获整体的代码信息。\n",
    "# 将代码分割为多个chunks\n",
    "tokens = tokenizer.tokenize(code_snippet)\n",
    "input_ids_chunks = [tokenizer.convert_tokens_to_ids(tokens[i:i+chunk_size]) for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "# 初始化一个列表来存储每个chunk的embedding\n",
    "chunk_embeddings = []\n",
    "\n",
    "for chunk in input_ids_chunks:\n",
    "    # 将chunk转换为tensor，并添加批次维度\n",
    "    input_ids_tensor = torch.tensor([chunk])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 通过模型获取输出\n",
    "        outputs = model(input_ids=input_ids_tensor)\n",
    "        # 获取最后一层的隐藏状态作为embedding\n",
    "        chunk_embeddings.append(outputs.last_hidden_state.mean(dim=1))\n",
    "\n",
    "# 计算所有chunk的embeddings的平均值\n",
    "average_embedding = torch.mean(torch.stack(chunk_embeddings), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.8975e-01,  6.4742e-02,  2.7215e-01,  1.1820e-01, -2.4820e-01,\n",
       "         -7.1653e-01, -1.9560e-02,  3.4860e-01,  5.0434e-01,  4.0049e-01,\n",
       "         -3.4864e-01,  8.6760e-01, -2.3258e-01, -1.3339e-01,  8.2739e-01,\n",
       "         -2.1255e-01,  1.7405e-01,  3.5642e-01, -3.8591e-02,  5.6129e-02,\n",
       "         -2.0588e-01, -2.7615e-01,  6.3956e-01, -5.2021e-01,  4.6999e-01,\n",
       "          4.3322e-01, -7.4934e-02,  8.6133e-01, -7.2590e-01,  8.3926e-01,\n",
       "         -8.2937e-02,  1.3590e-01,  1.3960e+00,  1.9649e-01,  3.8412e-01,\n",
       "         -3.7863e-01, -4.5276e-01,  1.8557e-01,  5.8113e-02, -3.6326e-01,\n",
       "          2.3319e-02,  6.1426e-01, -9.7316e-01, -1.3430e-01,  4.3775e-01,\n",
       "          3.0119e-01,  6.3022e-01, -8.0048e-02,  1.3813e-01,  6.5626e-01,\n",
       "          5.5321e-01,  1.6538e-01, -5.5476e-01, -3.6035e-01,  4.7931e-01,\n",
       "          5.6815e-01, -1.1156e+00, -8.2188e-01, -2.0880e-01, -4.3521e-01,\n",
       "         -6.7213e-02, -4.1730e-01, -3.5018e-01, -9.1060e-02,  1.3303e+00,\n",
       "          2.8497e-01,  7.0275e-01,  7.0493e-01, -1.8786e-01,  2.0532e-01,\n",
       "         -3.3356e-01, -3.8140e-01, -2.2363e-01, -6.7323e-01, -7.1561e-01,\n",
       "          7.7084e-01, -3.0160e-01, -4.3732e+00,  3.7421e-01,  4.9252e-01,\n",
       "          2.4839e-01, -5.9908e-01,  1.5054e+00,  6.0192e-01, -6.6425e-01,\n",
       "          3.1797e-01,  1.4749e-01,  1.5745e-01, -7.4243e-01, -5.9266e-02,\n",
       "          3.4953e-01, -2.3293e-02,  6.3874e-01,  6.7161e-01, -2.0371e-01,\n",
       "          8.1801e-01,  7.4050e-01, -6.0924e-01,  8.1733e-03, -4.4996e-01,\n",
       "          3.1258e-02, -7.5971e-01,  5.3449e-01,  4.7921e-01,  4.4158e-01,\n",
       "         -6.2849e-01,  5.4161e-01, -7.7288e-01,  5.1508e-01, -3.2857e-01,\n",
       "          1.0934e-01, -4.4336e-01,  8.5024e-01,  2.1595e-01,  3.2047e-01,\n",
       "         -4.7074e-01,  3.5386e-01,  1.5446e-01,  1.6232e-01,  6.7021e-02,\n",
       "         -8.5124e-01,  4.2684e-01, -2.1385e-01,  8.8915e-01, -4.7379e-01,\n",
       "          2.3907e-01, -4.4832e-02, -3.9925e-02,  1.1371e-01,  3.0077e-01,\n",
       "         -1.0761e+00, -7.5507e-01, -4.9792e-01,  6.0334e-01,  6.6202e-01,\n",
       "         -5.4791e-01,  4.2458e-01, -5.2859e-02, -3.9614e-01,  3.3966e-01,\n",
       "         -6.3124e-01, -6.1416e-01, -2.4697e-01,  1.6363e-01,  9.3814e-01,\n",
       "         -1.0143e-01,  2.8149e-04,  5.2746e-01,  2.3264e-01, -3.8307e-01,\n",
       "         -7.5672e-01, -4.2960e-02,  1.2872e+00, -1.6723e-01, -3.1591e-02,\n",
       "         -2.1242e+00,  1.4854e-01,  2.4414e-01,  2.1681e-01, -3.7604e-01,\n",
       "          3.0976e-01, -4.1757e-02, -4.1650e-02,  5.8748e-01,  6.3023e-01,\n",
       "          1.8146e-01, -4.9383e-01, -3.7774e-02,  2.7593e-01,  1.0510e+00,\n",
       "         -3.8981e-01, -5.7628e-01, -6.6612e-01,  6.1847e-02,  3.3355e-01,\n",
       "          8.7509e-01,  3.5433e-01, -2.0768e-01, -4.8974e-01,  1.1699e+00,\n",
       "          5.0248e-01, -1.9494e-01,  3.6537e-01, -5.6494e-01, -4.3482e-01,\n",
       "          4.3305e-01, -4.0783e-01,  7.4956e-01, -3.0825e-01, -2.4482e-01,\n",
       "         -3.1317e-01, -4.2401e-01,  3.1080e-01,  5.8288e-01,  1.3824e-01,\n",
       "          8.4873e-02,  3.1517e-02,  2.7881e-01,  8.8885e-01,  1.2194e-01,\n",
       "         -4.0855e-01,  9.3478e-01,  4.0187e-01,  3.8082e-01, -1.8705e-01,\n",
       "          3.2410e-01, -3.6169e-01,  5.8599e-01,  6.8407e-01,  1.4257e+00,\n",
       "          1.8311e+00,  1.1264e-01,  1.8990e-01, -4.9252e-01, -1.0194e+00,\n",
       "          1.7008e-01, -7.0364e-01, -1.6621e+00, -5.7908e-01, -5.9294e-01,\n",
       "         -1.4548e+00,  2.4729e-01, -3.0438e-01, -2.5158e-01, -2.7497e-01,\n",
       "          6.8038e-02,  3.5602e-01, -1.7771e-01, -3.8981e-01,  4.5383e-01,\n",
       "         -5.2965e-01, -2.3611e-01, -1.0585e+00, -1.7182e-01,  2.1703e-02,\n",
       "         -2.5843e-01, -6.4857e-01,  4.1366e-01,  4.4774e-01, -8.8123e-01,\n",
       "         -1.5131e-01,  2.5711e-01,  5.7101e-01,  8.4474e-01,  4.6600e-02,\n",
       "         -7.2064e-01,  4.1108e-01,  4.4624e-01,  5.4116e-01,  6.7288e-01,\n",
       "          2.6326e-01,  5.3419e-01,  3.6664e-01, -3.0056e-01,  9.7675e-02,\n",
       "         -2.5459e-01, -2.0581e-01, -4.2243e-01,  9.3415e-01,  1.8651e+00,\n",
       "          3.5406e-01, -1.2777e-01,  6.9279e-01, -5.9146e-01,  4.2011e-01,\n",
       "         -2.3818e-02,  4.7044e-01,  3.4487e-01,  8.1670e-01,  4.0603e-01,\n",
       "          1.4189e+00,  3.5586e-01, -3.6526e-01,  1.5620e-01,  2.8562e-01,\n",
       "          1.6865e-01,  4.4078e-01,  1.4338e-01, -8.9708e-01, -3.0317e-01,\n",
       "         -3.6044e-01, -5.8928e-01,  5.7073e-01, -6.5448e-02, -1.7955e-01,\n",
       "         -2.9572e-01, -2.1764e-01,  6.9292e-01,  1.6526e-02, -1.3681e-01,\n",
       "          3.6806e-01, -4.8500e-01,  1.1221e+00,  5.6734e-02, -2.7146e-01,\n",
       "          4.3403e-01,  3.7672e-01,  5.0015e-01,  3.9067e-01, -5.6341e-01,\n",
       "          1.7482e-01,  4.0827e-01, -5.7860e-02,  1.2988e-01, -3.0675e-01,\n",
       "         -3.5096e-01, -2.6720e-01,  4.3269e-01,  3.5748e-01, -1.8975e-01,\n",
       "          3.0495e-01, -9.3763e-01, -1.3462e-01, -1.5912e-01,  4.0665e-01,\n",
       "         -1.0420e-02, -1.9131e-01,  5.0317e-01, -5.3390e-01,  4.0162e-01,\n",
       "          3.4959e-01, -1.0320e-01,  9.9176e-01, -1.0049e+00,  6.4089e-01,\n",
       "          7.6242e-01, -7.1899e-01, -8.6569e-01, -1.5247e+00, -1.3139e-01,\n",
       "         -1.1380e+00,  8.9341e-01,  5.9951e-01,  1.8355e+00, -7.0575e-01,\n",
       "         -3.9861e-01,  3.9099e-01, -4.6550e-01,  1.1552e-01, -5.5348e-01,\n",
       "         -1.0655e+00,  7.0084e-01,  4.2982e-01, -3.8200e-01,  3.7917e-01,\n",
       "          1.0343e+00, -4.3944e-01, -2.0349e-01,  1.2863e+00,  3.2789e-01,\n",
       "         -6.2269e-01, -8.2645e-01, -3.3076e-01, -3.2553e-01,  3.5307e-01,\n",
       "          2.0154e+00,  5.9333e-01,  1.1955e-01,  7.5490e-02, -4.3652e-01,\n",
       "         -2.3266e-01, -2.8940e-02,  2.9712e-01,  1.8477e+00,  6.0376e-01,\n",
       "         -1.1612e-01, -9.1581e-01,  1.5954e-01, -1.1454e-01,  4.2918e-01,\n",
       "         -1.0294e-02, -6.6361e-02,  3.0405e-02,  7.2959e-01,  2.2401e-01,\n",
       "         -1.8136e-02,  5.7654e-01, -2.6773e-01,  2.3035e-01, -9.9849e-03,\n",
       "         -6.7418e-01,  4.3045e-01,  1.4403e-01, -1.9503e-02,  4.4203e-01,\n",
       "         -1.8228e+00,  6.6320e-01, -4.2250e-01,  1.4278e+00, -7.3256e-02,\n",
       "          1.6640e-01,  3.3650e-01,  1.2112e-01, -3.9750e-01, -3.4551e-01,\n",
       "          3.4956e-01,  8.5572e-02, -3.5394e-01, -3.2459e-01, -5.1687e-01,\n",
       "         -4.2939e-01,  2.4225e-01, -1.3527e-01, -1.0150e-01, -1.5191e-01,\n",
       "         -7.1777e-02, -4.9729e-01, -7.0105e-02, -5.9535e-01,  6.5555e-01,\n",
       "         -4.3501e-01,  1.7229e+00, -2.2074e-01, -3.2949e-02, -1.5838e-01,\n",
       "          3.9190e-01,  6.5592e-01, -7.2087e-01,  2.5895e-01,  4.7309e-01,\n",
       "          1.1759e-01,  3.5325e-01,  6.0474e-01, -4.8180e-01,  7.7941e-01,\n",
       "         -2.2683e-01,  2.2096e-01, -4.3427e-02, -6.8811e-01, -5.9685e-01,\n",
       "         -4.9784e-01,  2.5694e-01, -3.0627e-02,  7.7132e-02, -8.1396e-01,\n",
       "         -3.3013e-01, -6.1554e-01, -3.3102e-01,  5.2609e-01,  3.9976e-01,\n",
       "          6.0391e-01,  2.6226e-01,  6.2445e-01,  2.7264e-01,  1.6483e-02,\n",
       "          4.6082e-01, -5.5614e-01,  1.5283e+00,  5.9130e-01, -7.4440e-01,\n",
       "          4.5554e-01,  1.4151e-01,  5.7280e-01, -1.5120e+00,  5.0806e-01,\n",
       "         -5.1583e-01,  1.7868e-01, -2.6420e-02,  5.2679e-01,  5.1487e-01,\n",
       "         -2.6290e-01,  1.6912e-01,  2.9227e-01, -2.2363e-01,  4.1196e-01,\n",
       "         -1.3960e+00,  4.4750e-01, -3.2952e-01,  4.9442e-01,  3.0309e-01,\n",
       "         -8.7255e-01,  1.7607e-01, -2.9893e-01, -4.4375e-01, -3.3896e-01,\n",
       "          1.3706e-02, -4.0581e-01,  1.8414e+00,  6.1463e-01,  1.5310e+00,\n",
       "         -9.2435e-02, -4.2215e-01, -3.2441e-01, -6.7423e-01,  8.0451e-01,\n",
       "          4.8416e-01, -9.9752e-02, -4.8345e-01,  5.0863e-02, -5.5911e-01,\n",
       "         -7.3193e-01, -3.8845e-01, -5.4714e-01,  9.2205e-01, -5.6966e-01,\n",
       "          1.9009e-01, -1.4633e+00,  7.9353e-01, -8.0646e-01,  7.6842e-01,\n",
       "          1.3356e-01,  1.9466e-01,  6.0633e-01,  1.4326e+00, -1.9761e-01,\n",
       "          4.2083e-01, -3.9695e-01,  1.0995e+00,  4.5346e-01, -1.0013e-01,\n",
       "         -1.3795e-01,  8.0404e-01,  1.1282e+00, -3.5616e-03, -1.0841e-01,\n",
       "          3.3709e-01, -4.9922e-01, -7.7136e-01, -1.0495e+00,  2.4450e+00,\n",
       "          6.2501e-01,  1.7157e-01, -2.6759e-01,  5.8037e-02,  1.6059e+00,\n",
       "         -3.1005e-01, -8.9034e-01, -1.0597e+00,  1.2816e-02, -6.6948e-01,\n",
       "         -5.2974e-01, -2.1020e-01,  6.0121e-01,  9.8595e-03,  3.1096e-01,\n",
       "         -3.2430e-01, -2.4383e-01,  1.3319e-02, -4.2436e-01,  4.1491e-01,\n",
       "         -6.2593e-01, -4.4654e-01,  1.7273e+00,  3.8808e-01,  5.2257e-01,\n",
       "         -3.2977e-01, -4.3560e-01, -1.6618e+00, -7.0390e-02, -4.3950e-01,\n",
       "          5.7021e-01,  4.3406e+00, -4.2351e-01, -5.3560e-01,  5.7269e-01,\n",
       "         -2.9130e-02, -1.6262e-01,  4.5845e-01, -2.4261e-01,  2.6831e-01,\n",
       "         -6.5682e-01,  8.7353e-01,  2.2697e-01, -5.3924e-01,  3.7877e-01,\n",
       "          1.1870e-01, -4.8400e-02, -2.0870e-01,  2.7451e-01, -3.9816e-01,\n",
       "         -1.4179e+00,  4.0634e-01,  4.6850e-01, -2.7785e-01,  4.0292e-01,\n",
       "         -1.7093e-01,  8.0629e-01, -3.1925e-01, -7.6058e-02,  5.5243e-03,\n",
       "          5.1530e-01,  1.4459e+00,  4.2105e-01,  3.4302e-01, -9.4546e-02,\n",
       "          7.7454e-01,  3.7945e-01,  4.7219e-01,  1.6479e+00, -4.6377e-01,\n",
       "          1.2568e+00,  4.6416e-01,  2.0535e-01, -1.2982e-01, -1.1698e+00,\n",
       "         -3.1778e-01, -2.2602e-01, -2.3042e-01, -4.1420e-01,  8.1013e-01,\n",
       "         -5.0761e-01, -2.0777e-01,  4.1966e-01, -6.6509e-01, -5.6152e-01,\n",
       "          2.7767e-01,  8.4984e-02, -1.3845e-01,  2.6363e-01,  1.3579e-01,\n",
       "          3.9653e-01, -1.2336e-01,  4.3743e-01, -9.5468e-02,  4.6426e-01,\n",
       "          4.4165e-01, -5.0715e-01, -2.8433e-01, -2.8301e-01,  4.2576e-01,\n",
       "         -3.0277e-01, -4.7337e-01, -6.0804e-01,  1.9082e+00,  2.2951e-01,\n",
       "         -6.6155e-01, -5.3561e-02, -1.3105e+00,  3.2150e-01, -4.1828e-01,\n",
       "          4.0904e-01, -4.7626e-01, -9.7180e-02, -1.6641e-01, -7.5761e-01,\n",
       "          5.0985e-01, -1.0033e-01, -2.2957e-01,  2.3403e-01,  6.4690e-01,\n",
       "         -5.0626e-01, -1.1140e-01,  5.9435e-01,  1.5901e-01, -1.0253e+00,\n",
       "         -7.5110e-01, -5.3015e-01, -2.6318e-01,  2.5686e-01,  6.9719e-01,\n",
       "          1.5575e-01, -1.1654e+00, -5.6877e-01, -2.2490e-01, -2.1089e-01,\n",
       "         -3.0993e-01,  4.3754e-01,  3.7938e-01,  1.6281e+00,  1.0188e+00,\n",
       "          2.3663e-01, -1.4580e-01, -4.1703e-01, -3.9653e-01,  1.2075e+00,\n",
       "         -7.0907e-01,  2.0154e-01, -8.1149e-01, -3.8648e-01,  5.4454e-01,\n",
       "         -6.1357e-01, -5.3292e-01,  5.4032e-01,  3.1428e-01,  9.4744e-01,\n",
       "          3.9189e-01,  1.5332e+00, -4.0745e-01,  3.9564e-01, -1.2691e-02,\n",
       "         -9.6924e-01,  8.8954e-02, -1.6172e-01, -4.3819e-01, -1.3139e+00,\n",
       "          1.0028e-01,  6.7494e-01,  1.6029e-02,  8.3716e-01, -5.5899e-01,\n",
       "          2.6907e-01,  2.6845e-01,  3.9496e-02, -7.3570e-01,  7.3779e-01,\n",
       "         -1.4612e-01,  7.7234e-01,  1.7767e-01, -1.2217e-01, -2.9050e-01,\n",
       "          1.0483e-01,  2.9802e-01,  1.7088e+00, -4.4362e-01, -1.4717e-01,\n",
       "          6.3886e-01,  1.7808e-01, -3.3849e-03,  4.1879e-01,  1.2493e+00,\n",
       "         -2.0403e-01, -3.9465e-01,  2.6158e-01, -9.9814e-02, -2.0104e-01,\n",
       "         -2.3115e-01,  1.1564e-01,  7.5221e-01, -2.8444e-01,  4.5982e-01,\n",
       "          1.0842e+00,  1.3354e+00,  7.8920e-01, -4.0686e-01, -9.0002e-01,\n",
       "         -9.0954e-02,  6.2415e-01,  3.4202e-01,  1.8382e-01, -2.5851e-01,\n",
       "          2.8559e-01, -6.5438e-01, -1.1951e+00, -5.4585e-01,  6.4639e-01,\n",
       "          2.4941e-01, -1.0714e-01, -4.0749e-01, -4.5584e-01,  7.8114e-01,\n",
       "         -3.8893e-01,  5.2245e-01,  3.8295e-01, -4.9535e-01,  5.2036e-02,\n",
       "          1.4097e+00,  2.2709e-01, -6.7113e-01,  1.0332e-01, -1.2412e+00,\n",
       "          2.9425e-01, -3.0199e-01,  3.1514e-01,  2.1171e-01,  5.2929e-01,\n",
       "         -2.9225e-01,  4.4790e-01, -1.9466e-01,  4.1662e-01, -2.3905e-01,\n",
       "         -5.7363e-01,  4.2077e-01,  8.1168e-03,  2.4399e-01,  1.0033e+00,\n",
       "         -9.1814e-01, -3.7683e-01,  5.5896e-01]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.13820284605026245\n",
      "Predicted label: 0.5388351082801819, Actual label: 0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "#average\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch import nn\n",
    "\n",
    "# 加载数据\n",
    "data = pd.DataFrame({\n",
    "    \"text\": [code_snippet, code_snippet*2, code_snippet*3],\n",
    "    \"label\": [0.8, 0.9, 0.2]\n",
    "})\n",
    "\n",
    "# 定义数据集\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# 初始化tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# 准备数据集\n",
    "train_data, val_data = train_test_split(data, test_size=0.1)\n",
    "train_dataset = SentimentDataset(train_data['text'].to_numpy(), train_data['label'].to_numpy(), tokenizer)\n",
    "val_dataset = SentimentDataset(val_data['text'].to_numpy(), val_data['label'].to_numpy(), tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "# 定义BERT回归模型\n",
    "class BertRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertRegressor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 检测是否有多个chunks\n",
    "        if input_ids.size(1) > 512:  # 假设batch_size在第一维\n",
    "            all_embeddings = []\n",
    "            step_size = 512\n",
    "            for i in range(0, input_ids.size(1), step_size):\n",
    "                chunk_input_ids = input_ids[:, i:i+step_size]\n",
    "                chunk_attention_mask = attention_mask[:, i:i+step_size]\n",
    "                chunk_outputs = self.bert(input_ids=chunk_input_ids, attention_mask=chunk_attention_mask)\n",
    "                chunk_embeddings = chunk_outputs.pooler_output\n",
    "                all_embeddings.append(chunk_embeddings)\n",
    "            \n",
    "            # 计算所有chunks embeddings的平均值\n",
    "            embeddings = torch.mean(torch.stack(all_embeddings, dim=0), dim=0)\n",
    "        else:\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = outputs.pooler_output\n",
    "        \n",
    "        return self.regressor(embeddings)\n",
    "\n",
    "model = BertRegressor()\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 训练模型\n",
    "model.train()\n",
    "for epoch in range(1):  # 这里的epoch数仅为示例，根据实际情况调整\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "# 假设outputs是模型的logits输出\n",
    "# 转换为预测的类别\n",
    "_, predicted_labels = torch.max(outputs, dim=1)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = (predicted_labels == labels).float().mean()\n",
    "\n",
    "# 简化的评估和预测步骤（根据需要实现完整的评估逻辑）\n",
    "model.eval()\n",
    "for batch in val_loader:\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        print(f\"Predicted label: {outputs.squeeze().item()}, Actual label: {batch['labels'].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.12283027917146683\n",
      "Predicted label: 0.46814465522766113, Actual label: 0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "#not average\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch import nn\n",
    "\n",
    "# 加载数据\n",
    "data = pd.DataFrame({\n",
    "    \"text\": [code_snippet, code_snippet*2, code_snippet*3],\n",
    "    \"label\": [0.8, 0.9, 0.2]\n",
    "})\n",
    "\n",
    "# 定义数据集\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# 初始化tokenizer\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# 准备数据集\n",
    "train_data, val_data = train_test_split(data, test_size=0.1)\n",
    "train_dataset = SentimentDataset(train_data['text'].to_numpy(), train_data['label'].to_numpy(), tokenizer)\n",
    "val_dataset = SentimentDataset(val_data['text'].to_numpy(), val_data['label'].to_numpy(), tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "# 定义BERT回归模型\n",
    "class BertRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertRegressor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.regressor(pooled_output)\n",
    "\n",
    "model = BertRegressor()\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 训练模型\n",
    "model.train()\n",
    "for epoch in range(1):  # 这里的epoch数仅为示例，根据实际情况调整\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "# 假设outputs是模型的logits输出\n",
    "# 转换为预测的类别\n",
    "_, predicted_labels = torch.max(outputs, dim=1)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = (predicted_labels == labels).float().mean()\n",
    "\n",
    "# 简化的评估和预测步骤（根据需要实现完整的评估逻辑）\n",
    "model.eval()\n",
    "for batch in val_loader:\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        print(f\"Predicted label: {outputs.squeeze().item()}, Actual label: {batch['labels'].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
